{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc773a1-a6e1-4e72-9434-dc3c0581607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# Standard library imports\n",
    "import math  # Math functions\n",
    "import os  # Operating system-related functions\n",
    "import pickle  # Object serialization\n",
    "from datetime import datetime, timedelta  # Date and time handling\n",
    "\n",
    "# External library imports\n",
    "import numpy as np  # Numerical operations\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import matplotlib.pyplot as plt  # Plotting\n",
    "from matplotlib.patches import Wedge  # Patch shapes\n",
    "from matplotlib.colors import LogNorm  # Log normalization for colormaps\n",
    "from matplotlib.ticker import LogLocator, LogFormatterSciNotation  # Logarithmic tick handling\n",
    "import matplotlib.dates as mdates  # Date formatting for matplotlib plots\n",
    "import matplotlib.dates as md  # Alias for mdates\n",
    "from scipy.optimize import curve_fit  # Curve fittpiing\n",
    "from scipy.fft import fft, fftfreq  # Fast Fourier Transform\n",
    "from scipy.ndimage import gaussian_filter1d  # 1D Gaussian filter\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from cmap import Colormap\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Tecplot imports\n",
    "import tecplot as tp  # Tecplot library\n",
    "from tecplot.constant import PlotType  # Tecplot plot types\n",
    "\n",
    "# Other external libraries\n",
    "import re  # Regular expressions\n",
    "from os import listdir  # Directory listing\n",
    "from os.path import isfile, join  # Path operations\n",
    "from itertools import zip_longest  # Iterator for long zip\n",
    "from collections import defaultdict  # Dictionary subclass for default values\n",
    "import juliandate  # Julian date conversions\n",
    "from scipy.optimize import curve_fit\n",
    "#from astropy.time import Time  # Time conversions and handling\n",
    "\n",
    "# FOr wave analysis:\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import MultipleLocator, FixedLocator\n",
    "import bottleneck as bn\n",
    "import pywt\n",
    "import ssqueezepy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b88f61-0168-493b-a6c8-d18ffcfbbaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "amu = 1.67e-27\n",
    "m_p = amu\n",
    "k_b = 1.38e-23\n",
    "mu_0 = 1.257e-6\n",
    "R_M = 2440\n",
    "M_M = -200.9 #nT R_M^3\n",
    "e = 1.602e-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0fb5cc-3b5f-4ac4-a243-39e8e3a6facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "\n",
    "# Load a PDS dataset and return a big numpy array with all the data\n",
    "def read_mag(path,highres=False):\n",
    "    # First, get all the files in the directory\n",
    "    all_files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    # Then use regex to pick out the .tab file\n",
    "    for i in all_files:\n",
    "        match = re.search(\".*\\.TAB\",i)\n",
    "        if match != None:\n",
    "            file = str(path+\"/\"+i)\n",
    "    if highres:\n",
    "        print(\"Reading highres data:\",path)\n",
    "        colnames=['year','DAY_OF_YEAR','hour','minute','second','TIME_TAG','X','Y','Z','B_x','B_y','B_z']\n",
    "    else:\n",
    "        print(\"Reading lowres data:\",path)\n",
    "        colnames=['year','DAY_OF_YEAR','hour','minute','second','TIME_TAG','NAVG','X','Y','Z','B_x','B_y','B_z','DBX','DBY','DBZ']\n",
    "    df = pd.read_csv(file, delimiter=r\"\\s+\", names=colnames, header=None)\n",
    "    return df\n",
    "\n",
    "def calc_B_xdip(X,Y,Z):\n",
    "    # COORDINATES IN KM, AS READ IN FROM DATA\n",
    "    # OUTPUT IN NT\n",
    "    Z1 = Z - 0.2*R_M\n",
    "    R = np.sqrt(X**2+Y**2+Z1**2)\n",
    "    return 3*M_M * X * Z1/R**5 * (R_M**3)\n",
    "\n",
    "def calc_B_ydip(X,Y,Z):\n",
    "    # COORDINATES IN KM, AS READ IN FROM DATA\n",
    "    # OUTPUT IN NT\n",
    "    Z1 = Z - 0.2*R_M\n",
    "    R = np.sqrt(X**2+Y**2+Z1**2)\n",
    "    return 3*M_M * Y * Z1/R**5 * (R_M**3)\n",
    "\n",
    "def calc_B_zdip(X,Y,Z):\n",
    "    # COORDINATES IN KM, AS READ IN FROM DATA\n",
    "    # OUTPUT IN NT\n",
    "    Z1 = Z - 0.2*R_M\n",
    "    R = np.sqrt(X**2+Y**2+Z1**2)\n",
    "    return M_M * (3*Z1**2 - R**2) / R**5 * (R_M**3)\n",
    "\n",
    "# Read in GRS and do the huge amount of work required to turn the data into something usable. summing states how many channels to summing over (i.e summing=100 means 100*0.01ms = 1s)\n",
    "\n",
    "def read_GRS(path,summing = 6000):\n",
    "    # Define the dataframe we actually want/need\n",
    "    template = {'timestamp': [], 'counts': [] } #'year': [], 'month': [], 'day': [], 'hour': [], 'minute': [], 'second': [], 'counts': []}\n",
    "    GRS = pd.DataFrame(data=template)\n",
    "    # First, get all the subfolders (representing months)\n",
    "    # months = [ f.path for f in os.scandir(path) if f.is_dir() ]\n",
    "    # Iterate through all months\n",
    "    # for month_dir in months: \n",
    "    all_files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    # Iterate through all .tab files\n",
    "    for i in all_files:\n",
    "        match = re.search(\".*\\.tab\",i)\n",
    "        if match != None:\n",
    "            # Get filename\n",
    "            file = str(path+i)\n",
    "            print(\"reading file:\",file)\n",
    "            # Read in data as temp, ugly dataframe\n",
    "            data = pd.read_csv(file, delimiter = r\"\\s+\", header=None)\n",
    "            #print(data)\n",
    "            # Step through each row, representing 1800s of data (which for some reason is stored as many columns rather than subsequent rows)\n",
    "            for batch in range(len(data[1])):\n",
    "                if batch%100==0:\n",
    "                    print(\"processing batch#\",batch)\n",
    "                # Convert to a useable time system\n",
    "                ye,mo,da,ho,mi,se,micro = juliandate.to_gregorian(data[1][batch]) #(note micro is microseconds, which we subsequently convert to milliseconds)\n",
    "                # Carefully structure the time object\n",
    "                base_time = datetime(year=ye, month=mo, day=da, hour=ho, minute=mi, second=int(se), microsecond=int(micro))\n",
    "                for channel in np.arange(91,16425-summing,summing): # This loops over each n=summing channels to group together. Skip first 50.\n",
    "                    sum_counts = 0\n",
    "                    zeros = 0\n",
    "                    for sum_channel in range(summing):\n",
    "                        sum_counts+=data[(channel+sum_channel)][batch]\n",
    "                        if data[(channel+sum_channel)][batch]==0:\n",
    "                            zeros+=1\n",
    "                    # Create a timedelta for the milliseconds\n",
    "                    delta = timedelta(milliseconds=int((channel-41)*10))\n",
    "                    # Add the timedelta to the base time\n",
    "                    new_time = base_time + delta\n",
    "                    #print(\"base time:\",base_time)\n",
    "                    #print(\"new time:\",new_time)\n",
    "                    # Sae\n",
    "                    if zeros==0: # Quick fix to deal with the complicated fact that new batches are started before old one finished\n",
    "                        GRS.loc[len(GRS)] = [new_time,sum_counts]\n",
    "                    #GRS.loc[len(GRS)] = [new_time,ye,mo,da,new_time.hour,new_time.minute,new_time.second + new_time.microsecond / 1_000_000,sum_counts]\n",
    "    return GRS\n",
    "\n",
    "# Read in GRS and do the huge amount of work required to turn the data into something usable. summing states how many channels to summing over (i.e summing=100 means 100*0.01ms = 1s)\n",
    "def read_GRS2(path):\n",
    "    # Define the dataframe we actually want/need\n",
    "    template = {'timestamp': [], 'counts': [] } #'year': [], 'month': [], 'day': [], 'hour': [], 'minute': [], 'second': [], 'counts': []}\n",
    "    GRS = pd.DataFrame(data=template)\n",
    "    # First, get all the subfolders (representing months)\n",
    "    months = [ f.path for f in os.scandir(path) if f.is_dir() ]\n",
    "    # Iterate through all months\n",
    "    for month_dir in months: \n",
    "        all_files = [f for f in listdir(month_dir) if isfile(join(month_dir, f))]\n",
    "        # Iterate through all .tab files\n",
    "        for i in all_files:\n",
    "            match = re.search(\".*\\.tab\",i)\n",
    "            if match != None:\n",
    "                # Get filename\n",
    "                file = str(month_dir+\"/\"+i)\n",
    "                # Read in data as temp, ugly dataframe\n",
    "                data = pd.read_csv(file, delimiter = r\"\\s+\", header=None)\n",
    "                #print(data)\n",
    "                # Step through each row, representing 1800s of data (which for some reason is stored as many columns rather than subsequent rows)\n",
    "                for batch in range(len(data[1])):\n",
    "                    if batch%100==0:\n",
    "                        print(\"processing batch#\",batch)\n",
    "                    # Convert to a useable time system\n",
    "                    ye,mo,da,ho,mi,se,micro = juliandate.to_gregorian(data[1][batch]) #(note micro is microseconds, which we subsequently convert to milliseconds)\n",
    "                    base_time = datetime(year=ye, month=mo, day=da, hour=ho, minute=mi, second=int(se), microsecond=int(micro))\n",
    "                    GRS.loc[len(GRS)] = [base_time,data[16424][batch]]\n",
    "    return GRS\n",
    "        \n",
    "\n",
    "# Compute dipole field Bz\n",
    "def Bz_dip(x_array,y_array,z_array):\n",
    "    # Input: arrays of x,y,z (in planet centered coords).\n",
    "    # Output: Bz at each point\n",
    "    return - 200.9 * (3*(z_array-0.2)**2 - (x_array**2+y_array**2+(z_array-0.2)**2))/((x_array**2+y_array**2+(z_array-0.2)**2)**(5/2))\n",
    "\n",
    "# Read in magnetic field data from a simulation file\n",
    "def get_B_from_sim(filename,df):\n",
    "    # This function takes the path to a 3d tecplot file and a dataframe with X,Y,Z\n",
    "    # It returns the simulated B field at those points, which are added to the dataframe\n",
    "    \n",
    "    # Load data\n",
    "    tp.session.connect()\n",
    "    tp.new_layout()\n",
    "    dataset = tp.data.load_tecplot(filename)\n",
    "    fr = tp.active_frame()\n",
    "    fr.plot_type = PlotType.Cartesian3D\n",
    "\n",
    "    # See if there is already model data to overwrite:\n",
    "    if \"B_x_sim\" in df:\n",
    "        return df\n",
    "    #else:\n",
    "    # Define empty arrays\n",
    "    Bx = np.zeros_like(df[\"X\"])\n",
    "    By = np.zeros_like(df[\"X\"])\n",
    "    Bz = np.zeros_like(df[\"X\"])\n",
    "\n",
    "    # Pull data for each. Also pulls the one point before start_time, to fix the gradients.\n",
    "    for i in range(len(df)):\n",
    "        result = tp.data.query.probe_at_position(df[\"X\"].iloc[i]/R_M,df[\"Y\"].iloc[i]/R_M,df[\"Z\"].iloc[i]/R_M)\n",
    "        if result is not None:\n",
    "            Bx[i],By[i],Bz[i] = result[0][3:6] # Make sure to check that 3:6 corresponds to the B data!\n",
    "        # If this is outside the bounds of the simulation data, it will be set to 0\n",
    "\n",
    "    # Update values in df or add new columns\n",
    "    df.insert(len(df.columns), \"B_x_sim\",Bx)\n",
    "    df.insert(len(df.columns), \"B_y_sim\",By)\n",
    "    df.insert(len(df.columns), \"B_z_sim\",Bz)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Return a df of just the data around the current sheet centers, using either a fixed z window, or based on the Bx reversal\n",
    "def extract_cs(df,criteria = \"z\",threshold = 0.1):\n",
    "    # Extract all the data within the current sheet, as defined by the criteria\n",
    "    # Critera: z - choose all points within +- threshold of z = 0.2 R_M\n",
    "    if criteria == \"z\":\n",
    "        df_slice = df[(df['Z']/R_M>=(0.2-threshold)) & (df['Z']/R_M<=(0.2+threshold)) & (df['X']<0)].reset_index()\n",
    "        cs_pass = [0]\n",
    "        for index in range(1,len(df_slice)):\n",
    "            if df_slice[\"TIME_TAG\"][index]-df_slice[\"TIME_TAG\"][index-1]>1.1:\n",
    "                cs_pass.append(cs_pass[index-1]+1)\n",
    "            else:\n",
    "                cs_pass.append(cs_pass[index-1])\n",
    "        df_slice.insert(len(df_slice.columns), \"cs_pass\",cs_pass)\n",
    "\n",
    "        return df_slice\n",
    "\n",
    "    if criteria == \"Bx\":\n",
    "        # First, downselected to the regions near the current sheet. Add cs_pass to this area\n",
    "        df_slice = df[(df['Z']/R_M>=(0.2-3*threshold)) & (df['Z']/R_M<=(0.2+3*threshold)) & (df['X']<0)]\n",
    "        cs_pass = [0]\n",
    "        count=0\n",
    "        prev_index = df_slice.index.tolist()[0]\n",
    "        for index in df_slice.index.tolist()[1:]:\n",
    "            if index - prev_index >1: # If we have reached a skip in the indexing\n",
    "                count+=1\n",
    "            cs_pass.append(count)\n",
    "            prev_index = index\n",
    "        df_slice.insert(len(df_slice.columns), \"cs_pass\",cs_pass)\n",
    "        # Iterate through each pass to further filter\n",
    "        all_indices = np.array(())\n",
    "        for pass_num in np.unique(df_slice[\"cs_pass\"]):\n",
    "            #print(\"Looking for center of cs pass\",pass_num)\n",
    "            df_pass = df_slice[df_slice[\"cs_pass\"]==pass_num]\n",
    "            # Then find where Bx is minimum and is negative before and positive after\n",
    "            # Take the square of the values so that its just magnitude\n",
    "            Bx_vals = np.unique(df_pass[\"B_x\"]**2)\n",
    "            # Iterate through each value, from smallest to largest until a suitable value is found\n",
    "            count=0\n",
    "            while count>=0:\n",
    "                index = df_pass.index[df_pass['B_x']**2 == Bx_vals[count]][0]\n",
    "                if (df_slice[\"B_x\"][index-1]<0) and (df_slice[\"B_x\"][index+1]>0):\n",
    "                    #print(\"Match! CS found for pass\",pass_num,'at Z =',df_pass[\"Z\"][index]/R_M)\n",
    "                    # If this condition is met, find the indices corresponding to this\n",
    "                    indices = df_pass.index[(df_pass[\"Z\"] - df_pass[\"Z\"][index])**2 < (threshold*R_M)**2].tolist() # Factor of 2 to increase how much data we return\n",
    "                    all_indices = np.concatenate((all_indices, indices), axis=None)\n",
    "                    count=-1\n",
    "                else:\n",
    "                    count+=1   \n",
    "        return df_slice.loc[all_indices]\n",
    "\n",
    "    if criteria == \"harris\":\n",
    "        # Fit the Bx to a harris current sheet\n",
    "        # First, cut down the data to the vicinity of current sheet crossings\n",
    "        df_crossings = df[(df['Z']/R_M>=(0.2-3*threshold)) & (df['Z']/R_M<=(0.2+3*threshold)) & (df['X']<0)]\n",
    "        cs_pass = [0]\n",
    "        count=0\n",
    "        prev_index = df_crossings.index.tolist()[0]\n",
    "        for index in df_crossings.index.tolist()[1:]:\n",
    "            if index - prev_index > 1: # If we have reached a skip in the indexing\n",
    "                count+=1\n",
    "            cs_pass.append(count)\n",
    "            prev_index = index\n",
    "        df_crossings.insert(len(df_crossings.columns), \"cs_pass\",cs_pass)\n",
    "\n",
    "        return df_crossings\n",
    "\n",
    "# All calculations of secondary terms are done here\n",
    "def calculate_terms(df,use_model_data=False):\n",
    "    # Compute spherical coords\n",
    "    R = np.sqrt(df['X']**2+df['Y']**2+(df['Z']-0.2*R_M)**2) # km\n",
    "    \n",
    "    # Compute magnetic field strength\n",
    "    B_mag = np.sqrt(df['B_x']**2+df['B_y']**2+df['B_z']**2) # nT\n",
    "\n",
    "    # Compute magnetic pressure\n",
    "    P_mag = B_mag**2/(2*mu_0)*1e-9 # nPa\n",
    "\n",
    "    # Compute dipole field, from https://ccmc.gsfc.nasa.gov/static/files/Dipole.pdf\n",
    "    B_xdip = 3*M_M * df['X']/R_M * (df['Z']-0.2*R_M)/R_M/(R/R_M)**5\n",
    "    B_ydip = 3*M_M * df['Y']/R_M * (df['Z']-0.2*R_M)/R_M/(R/R_M)**5\n",
    "    B_zdip = M_M * (3*((df['Z']-0.2*R_M)/R_M)**2-(R/R_M)**2)/(R/R_M)**5\n",
    "    B_dip = np.sqrt(B_xdip**2+B_ydip**2+B_zdip**2)\n",
    "    P_magdip = B_dip**2/(2*mu_0)*1e-9 # Magnetic pressure of dipole field\n",
    "    \n",
    "    # Compute local lat/time\n",
    "    lat = np.arctan(df['Z']/np.sqrt(df['X']**2+df['Y']**2))*180/np.pi\n",
    "    phi = np.arctan(df['Y']/df['X'])\n",
    "    local_time = phi*12/np.pi + 12 \n",
    "\n",
    "    # Compute timestamp\n",
    "    year = df['year'].to_numpy()\n",
    "    daynum = df['DAY_OF_YEAR'].to_numpy()\n",
    "    month_ls = []\n",
    "    day_ls = []\n",
    "    for i in range(len(year)):\n",
    "        date = datetime.strptime(str(year[i]) + \"-\" + str(daynum[i]), \"%Y-%j\").strftime(\"%m-%d-%Y\")\n",
    "        month_ls.append(date[0:2])\n",
    "        day_ls.append(date[3:5])\n",
    "\n",
    "    # Pull MHD result values, saving timestamp first so it knows how to read the right times\n",
    "    df.insert(len(df.columns), \"day\",day_ls)\n",
    "    df.insert(len(df.columns), \"month\",month_ls)\n",
    "    df.insert(0, \"timestamp\", pd.to_datetime(df[['year','day','month','hour','minute','second']]))\n",
    "    if use_model_data:\n",
    "        df = get_B_from_MHD(GM_output,df,start_time,end_time,\"GM\")\n",
    "        B_xMHD = df['B_xMHD']\n",
    "        B_yMHD = df['B_yMHD']\n",
    "        B_zMHD = df['B_zMHD']\n",
    "        B_MHD = np.sqrt(B_xMHD**2+B_yMHD**2+B_zMHD**2)\n",
    "        P_magMHD = B_MHD**2/(2*mu_0)*1e-9 # Magnetic pressure of MHD result\n",
    "    \n",
    "    # Compute gradients:\n",
    "    # Step through each cell and compute dx,dy,dz. We skip the first cell.\n",
    "    dx=[0]\n",
    "    dy=[0]\n",
    "    dz=[0]\n",
    "    dBx=[0]\n",
    "    dBy=[0]\n",
    "    dBz=[0]\n",
    "    dB_xdip=[0]\n",
    "    dB_ydip=[0]\n",
    "    dB_zdip=[0]\n",
    "    dP_mag=[0]\n",
    "    dP_magdip=[0]\n",
    "    if use_model_data:\n",
    "        dB_xMHD=[0]\n",
    "        dB_yMHD=[0]\n",
    "        dB_zMHD=[0]\n",
    "        dP_magMHD=[0]\n",
    "    for i in range(1,len(df)):\n",
    "        dx.append(df['X'][i]-df['X'][i-1]) #dx\n",
    "        dy.append(df['Y'][i]-df['Y'][i-1])\n",
    "        dz.append(df['Z'][i]-df['Z'][i-1])\n",
    "        dBx.append(df['B_x'][i]-df['B_x'][i-1]) #dBx\n",
    "        dBy.append(df['B_y'][i]-df['B_y'][i-1])\n",
    "        dBz.append(df['B_z'][i]-df['B_z'][i-1])\n",
    "        dB_xdip.append(B_xdip[i]-B_xdip[i-1]) #dBx of dipole field\n",
    "        dB_ydip.append(B_ydip[i]-B_ydip[i-1])\n",
    "        dB_zdip.append(B_zdip[i]-B_zdip[i-1])\n",
    "        dP_mag.append(P_mag[i]-P_mag[i-1]) #dP_mag\n",
    "        dP_magdip.append(P_magdip[i]-P_magdip[i-1]) #dP_mag of dipole field\n",
    "        if use_model_data:\n",
    "            dB_xMHD.append(B_xMHD[i]-B_xMHD[i-1]) #dBx of MHD result\n",
    "            dB_yMHD.append(B_yMHD[i]-B_yMHD[i-1])\n",
    "            dB_zMHD.append(B_zMHD[i]-B_zMHD[i-1])\n",
    "            dP_magMHD.append(P_magMHD[i]-P_magMHD[i-1]) #dP_mag of MHD result\n",
    "        #  Set the 0th element equal to the first\n",
    "        if i==1:\n",
    "            dx[0] = dx[i]\n",
    "            dy[0] = dy[i]\n",
    "            dz[0] = dz[i]\n",
    "            dBx[0] = dBx[i]\n",
    "            dBy[0] = dBy[i]\n",
    "            dBz[0] = dBz[i]\n",
    "            dB_xdip[0] = dB_xdip[i]\n",
    "            dB_ydip[0] = dB_ydip[i]\n",
    "            dB_zdip[0] = dB_zdip[i]\n",
    "            dP_mag[0] = dP_mag[i]\n",
    "            dP_magdip[0] = dP_magdip[i]\n",
    "            if use_model_data:\n",
    "                dB_xMHD[0] = dB_xMHD[i]\n",
    "                dB_yMHD[0] = dB_yMHD[i]\n",
    "                dB_zMHD[0] = dB_zMHD[i]\n",
    "                dP_magMHD[0] = dP_magMHD[i]\n",
    "    # Save to df\n",
    "    df.insert(len(df.columns), \"lat\",lat)\n",
    "    df.insert(len(df.columns), \"local_time\",local_time) \n",
    "    df.insert(len(df.columns), \"B_mag\",B_mag)\n",
    "    df.insert(len(df.columns), \"P_mag\",P_mag)\n",
    "    df.insert(len(df.columns), \"B_xdip\",B_xdip)\n",
    "    df.insert(len(df.columns), \"B_ydip\",B_ydip)\n",
    "    df.insert(len(df.columns), \"B_zdip\",B_zdip)\n",
    "    df.insert(len(df.columns), \"B_dip\",B_dip)\n",
    "    df.insert(len(df.columns), \"P_magdip\",P_magdip)\n",
    "    df.insert(len(df.columns), \"dX\",dx)\n",
    "    df.insert(len(df.columns), \"dY\",dy)\n",
    "    df.insert(len(df.columns), \"dZ\",dz)\n",
    "    df.insert(len(df.columns), \"dB_x\",dBx)\n",
    "    df.insert(len(df.columns), \"dB_y\",dBy)\n",
    "    df.insert(len(df.columns), \"dB_z\",dBz)\n",
    "    df.insert(len(df.columns), \"dB_xdip\",dB_xdip)\n",
    "    df.insert(len(df.columns), \"dB_ydip\",dB_ydip)\n",
    "    df.insert(len(df.columns), \"dB_zdip\",dB_zdip)\n",
    "    df.insert(len(df.columns), \"dP_mag\",dP_mag)\n",
    "    df.insert(len(df.columns), \"dP_magdip\",dP_magdip)\n",
    "    if use_model_data:\n",
    "        df.insert(len(df.columns), \"B_MHD\",B_MHD)\n",
    "        df.insert(len(df.columns), \"P_magMHD\",P_magMHD)\n",
    "        df.insert(len(df.columns), \"dB_xMHD\",dB_xMHD)\n",
    "        df.insert(len(df.columns), \"dB_yMHD\",dB_yMHD)\n",
    "        df.insert(len(df.columns), \"dB_zMHD\",dB_zMHD)\n",
    "        df.insert(len(df.columns), \"dP_magMHD\",dP_magMHD)\n",
    "    \n",
    "    # Make corrections\n",
    "    df['local_time'][df['X']<0] = 24 - df['local_time'][df['X']<0]\n",
    "    return df\n",
    "\n",
    "def plot_cs_passes4(df,df_cs,colorbar=\"Bz1\"):\n",
    "    # Makes a nice plot of the current sheet crossings, like Jim's\n",
    "\n",
    "    # Determine which paths are in front of the planet vs. behind\n",
    "    df_noview =df[(df['X']>0)]\n",
    "    df_inview =df[(df['X']<0)]# & (df['timestamp']>pd.to_datetime(\"2015-04-01 21:00:00\")) & (df['timestamp']<pd.to_datetime(\"2015-04-03 13:00:00\"))]\n",
    "    \n",
    "    # Define figure and patches\n",
    "    fig, ax = plt.subplots(ncols=1,figsize=(7,5))\n",
    "    p0 = plt.Circle((0, 0), 1, color='black')\n",
    "    p1 = plt.Circle((0, 0), 0.99, color='lightgrey')\n",
    "\n",
    "    plot0=ax.scatter(df_noview[\"Y\"]/R_M,df_noview[\"Z\"]/R_M,s=0.01,color='black',alpha=0.5)\n",
    "    \n",
    "    # Put in the planet\n",
    "    ax.add_artist(p0)\n",
    "    ax.add_artist(p1)\n",
    "\n",
    "    # plot\n",
    "    plot1=ax.scatter(df_inview[\"Y\"]/R_M,df_inview[\"Z\"]/R_M,s=0.01,color='black',alpha=0.5)#,c=list((df_inview['DAY_OF_YEAR']-np.min(df_inview['DAY_OF_YEAR']))),\n",
    "    ax.plot([-1,1],[0.2,0.2],linestyle=\"dashed\",color=\"black\")\n",
    "    for pass_num in range(np.max(df_cs['cs_pass'])+1):\n",
    "        df_pass = df_cs[(df_cs['cs_pass']==pass_num)]\n",
    "        midpoint = [df_pass[\"Y\"][df_pass.index[len(df_pass.index)//2]]/R_M, df_pass[\"Z\"][df_pass.index[len(df_pass.index)//2]]/R_M]\n",
    "        if colorbar == \"Bz1\":\n",
    "            plot2=ax.scatter(df_pass[\"Y\"]/R_M,df_pass[\"Z\"]/R_M,s=1.5,c = df_pass[\"B_z\"]-df_pass[\"B_zdip\"],vmin=-50,vmax=50,cmap='bwr')\n",
    "            ax.plot([midpoint[0]-0.008,midpoint[0]+0.008],[midpoint[1],midpoint[1]],lw=1.8,color=\"yellow\")\n",
    "        elif colorbar == \"alt\":\n",
    "            alt = np.sqrt(df_pass[\"X\"]**2+df_pass[\"Y\"]**2+df_pass[\"Z\"]**2)/R_M - 1\n",
    "            plot2=ax.scatter(df_pass[\"Y\"]/R_M,df_pass[\"Z\"]/R_M,s=1.5,c = alt,cmap='rainbow',vmin=0.1,vmax=0.3)\n",
    "            ax.plot([midpoint[0]-0.008,midpoint[0]+0.008],[midpoint[1],midpoint[1]],lw=1.8,color=\"yellow\")\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    ax.set_xlim(-1.2,1.2)\n",
    "    ax.set_ylim(-1.2,1.2)\n",
    "    ax.invert_xaxis()\n",
    "    ax.set_xlabel(\"Y [$R_M$]\")\n",
    "    ax.set_ylabel(\"Z [$R_M$]\")\n",
    "    clb1 = fig.colorbar(plot2, ax=ax)\n",
    "    if colorbar == \"Bz1\":\n",
    "        clb1.ax.set_title(\"$B_{z,obs}-B_{z,dip}$ [nT]\")\n",
    "    if colorbar == \"alt\":\n",
    "        clb1.ax.set_title(\"Altitude [$R_M$]\")\n",
    "    plt.savefig(\"MESSENGER_orbits.png\",dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_Bz_minus_Bzdip(df_cs):\n",
    "    # For a df_cs dataframe, generate a plot of Bz1=Bz-Bzdip for each pass to help identify DR events\n",
    "    count=0\n",
    "    for i in np.unique(df_cs[\"cs_pass\"]): #[5,10,27,32]: \n",
    "        indices = np.where(df_cs[\"cs_pass\"]==i)[0]\n",
    "        plt.figure(figsize=(10,6))\n",
    "        #plt.plot(df_cs['timestamp'][indices],df_cs[\"B_mag\"][indices],color=\"black\")\n",
    "        #plt.plot(df_cs['timestamp'][indices],df_cs[\"B_dip\"][indices],color=\"black\",linestyle=\"dashed\")\n",
    "        plt.plot(df_cs['Z'][indices],df_cs[\"B_z\"][indices]-df_cs[\"B_zdip\"][indices],color=\"black\")\n",
    "        plt.plot(df_cs['Z'][indices],df_cs[\"B_zdip\"][indices]*0,color=\"black\",linestyle=\"dashed\")\n",
    "        plt.title(str(\"CS pass: \"+str(i)+\"\\n starting on: \"+str(df_cs[\"timestamp\"][indices[0]])))\n",
    "        plt.ylim(-100,100)\n",
    "        if np.max(df_cs[\"B_z\"][indices]-df_cs[\"B_zdip\"][indices])>0:\n",
    "            count+=1\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "def extract_DR_passes(df_cs,max_DR_width = 0.4):\n",
    "    # For a df_cs dataframe, generate a plot of Bz1=Bz-Bzdip for each pass to help identify DR events\n",
    "    count=0\n",
    "    passes = {}\n",
    "    # Loop through all the current sheet passes\n",
    "    for i in np.unique(df_cs[\"cs_pass\"]): #[5,10,27,32]: \n",
    "        # Work out the indices which  correspond to the dataframe entries of this flyby\n",
    "        indices = np.where(df_cs[\"cs_pass\"]==i)[0]\n",
    "        # Work out the index of the cs center\n",
    "        midpoint = indices[len(indices)//2]\n",
    "        # Work out the indices of the max and min Z value by which Bz1<0 for DR classification \n",
    "        upper_indices = df_cs['Z'][indices].index[df_cs['Z'][indices] >= df_cs['Z'][midpoint]+max_DR_width*R_M/2].tolist()\n",
    "        lower_indices = df_cs['Z'][indices].index[df_cs['Z'][indices] <= df_cs['Z'][midpoint]-max_DR_width*R_M/2].tolist()\n",
    "        # In general, we don't know if we are flying south to north or vice versa; this section deals with that\n",
    "        if max(upper_indices)>max(lower_indices): # South to north case\n",
    "            izmax = upper_indices[0]\n",
    "            izmin = lower_indices[-1]\n",
    "        else:\n",
    "            izmax = upper_indices[-1]\n",
    "            izmin = lower_indices[0]\n",
    "\n",
    "        # Filter 1: Bz1>0 at cs center\n",
    "        if (df_cs[\"B_z\"][midpoint]>df_cs[\"B_zdip\"][midpoint]):\n",
    "            #print(\"cs pass\",i,\"is enhanced at current sheet center (\"+str(df_cs['Z'][midpoint]/R_M)+\")\")\n",
    "            # Filter 2: Bz1<0 at max width edges\n",
    "            if (df_cs[\"B_z\"][izmax]<df_cs[\"B_zdip\"][izmax]) and (df_cs[\"B_z\"][izmin]<df_cs[\"B_zdip\"][izmin]):\n",
    "                # Filter 3: Check it only peaks 1-2 times in this range\n",
    "                peak_count = 0\n",
    "                for j in np.arange(1+min(izmin,izmax),max(izmin,izmax)):\n",
    "                    if (df_cs[\"B_z\"][j-1]<df_cs[\"B_zdip\"][j-1]) and (df_cs[\"B_z\"][j]>df_cs[\"B_zdip\"][j]): # i.e if this index represents a change from Bz1<0 to Bz1>1, this is the start of a peak\n",
    "                        peak_count+=1\n",
    "                if peak_count<3:\n",
    "                    passes[i] = indices\n",
    "                '''\n",
    "                # Filter 3: Bz1<0 within max_DR_width/2 of the cs center (currently redundant)\n",
    "                if max(upper_indices)>max(lower_indices): # South to north case\n",
    "                    if (df_cs[\"B_z\"][midpoint:izmax]-df_cs[\"B_zdip\"][midpoint:izmax]).min()<0 and (df_cs[\"B_z\"][izmin:midpoint]-df_cs[\"B_zdip\"][izmin:midpoint]).min()<0:\n",
    "                        #print(\"cs pass\",i,\"also has the Bz1 decrease on the edges! It's a DR\")\n",
    "                        passes.append(i)\n",
    "                else: \n",
    "                    if (df_cs[\"B_z\"][izmax:midpoint]-df_cs[\"B_zdip\"][izmax:midpoint]).min()<0 and (df_cs[\"B_z\"][midpoint:izmin]-df_cs[\"B_zdip\"][midpoint:izmin]).min()<0:\n",
    "                        #print(\"cs pass\",i,\"also has the Bz1 decrease on the edges! It's a DR\")\n",
    "                        passes.append(i)\n",
    "                '''\n",
    "            \n",
    "            #plt.plot(df_cs['timestamp'][indices],df_cs[\"B_mag\"][indices],color=\"black\")\n",
    "        #plt.plot(df_cs['timestamp'][indices],df_cs[\"B_dip\"][indices],color=\"black\",linestyle=\"dashed\")\n",
    "        if i in passes:\n",
    "            color='blue'\n",
    "        else:\n",
    "            color='red'\n",
    "            \n",
    "        #print(\"Average R for this DR:\",np.mean(np.sqrt(df_cs[\"X\"][indices]**2+df_cs[\"Y\"][indices]**2+df_cs[\"Z\"][indices]**2)/R_M))\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.plot(df_cs['timestamp'][indices],df_cs[\"B_z\"][indices]-df_cs[\"B_zdip\"][indices],color=color)\n",
    "        plt.plot(df_cs['timestamp'][indices],df_cs[\"B_mag\"][indices],color=color)\n",
    "        plt.plot(df_cs['timestamp'][indices],df_cs[\"B_zdip\"][indices]*0,color='black')\n",
    "        plt.plot([df_cs['timestamp'][midpoint],df_cs['timestamp'][midpoint]],[-100,100],color='black',linestyle=\"dashed\")\n",
    "        plt.plot([df_cs['timestamp'][izmax],df_cs['timestamp'][izmax]],[-100,100],color='black',linestyle=\"dotted\")\n",
    "        plt.plot([df_cs['timestamp'][izmin],df_cs['timestamp'][izmin]],[-100,100],color='black',linestyle=\"dotted\")\n",
    "        plt.title(str(\"CS pass: \"+str(i)+\"\\n starting on: \"+str(df_cs[\"timestamp\"][indices[0]])))\n",
    "        #plt.ylim(-100,100)\n",
    "        if np.max(df_cs[\"B_z\"][indices]-df_cs[\"B_zdip\"][indices])>0:\n",
    "            count+=1\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    return passes\n",
    "\n",
    "def plot_GRS_and_DR(df_cs,GRS,DR_dict):\n",
    "    for DR in DR_dict.keys():\n",
    "        indices = DR_dict[DR]\n",
    "        GRS_indices = np.where((GRS[\"timestamp\"]>df_cs[\"timestamp\"][indices[0]]) & (GRS[\"timestamp\"]<df_cs[\"timestamp\"][indices[-1]]))[0]\n",
    "        midpoint = indices[len(indices)//2]\n",
    "        fig, ax = plt.subplots(figsize=(10,6))\n",
    "        #ax2 = ax.twinx()\n",
    "        ax.plot(df_cs['timestamp'][indices],df_cs[\"B_z\"][indices]-df_cs[\"B_zdip\"][indices],color='black')\n",
    "        ax.plot(GRS['timestamp'][GRS_indices],GRS[\"counts\"][GRS_indices],color='blue')\n",
    "        #ax2.yaxis.label.set_color('blue')\n",
    "        #ax2.tick_params(axis ='y', labelcolor = \"blue\")\n",
    "        ax.plot(df_cs['timestamp'][indices],df_cs[\"B_zdip\"][indices]*0,color='black')\n",
    "        ax.plot([df_cs['timestamp'][midpoint],df_cs['timestamp'][midpoint]],[-100,100],color='black',linestyle=\"dashed\")\n",
    "        ax.set_title(str(\"CS pass: \"+str(DR-1)+\"\\n starting on: \"+str(df_cs[\"timestamp\"][indices[0]])))\n",
    "        ax.set_ylim(-100,100)\n",
    "        #ax2.set_ylim(-10000,10000)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "def average_datasets_with_std(datasets, x_common=None, num_points=1000):\n",
    "    # Flatten all x values from datasets to determine the common x range\n",
    "    all_x = np.concatenate([x for x, z in datasets])\n",
    "    min_x, max_x = np.min(all_x), np.max(all_x)\n",
    "\n",
    "    # If a custom common x grid is not provided, generate it\n",
    "    if x_common is None:\n",
    "        x_common = np.linspace(min_x, max_x, num_points)\n",
    "\n",
    "    # Initialize arrays for the accumulated sums and squared differences\n",
    "    z_sum = np.zeros_like(x_common)\n",
    "    z_squared_sum = np.zeros_like(x_common)\n",
    "    valid_counts = np.zeros_like(x_common)\n",
    "\n",
    "    # Interpolate each dataset to the common x values and accumulate results\n",
    "    for x, z in datasets:\n",
    "        interp_func = interp1d(x, z, kind='linear', bounds_error=False, fill_value=np.nan)\n",
    "        z_interp = interp_func(x_common)\n",
    "\n",
    "        valid_mask = ~np.isnan(z_interp)\n",
    "\n",
    "        z_sum[valid_mask] += z_interp[valid_mask]\n",
    "        z_squared_sum[valid_mask] += z_interp[valid_mask] ** 2\n",
    "        valid_counts[valid_mask] += 1\n",
    "\n",
    "    # Compute average\n",
    "    with np.errstate(invalid='ignore', divide='ignore'):  # Suppress warnings\n",
    "        z_average = z_sum / valid_counts\n",
    "\n",
    "    # Compute variance and standard deviation\n",
    "    with np.errstate(invalid='ignore', divide='ignore'):  # Suppress warnings\n",
    "        z_variance = (z_squared_sum / valid_counts) - (z_average ** 2)\n",
    "        z_std = np.sqrt(z_variance)\n",
    "\n",
    "    return x_common, z_average, z_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0b53e5-9af4-442b-a5d5-2c9ce7e9d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jiutong Zhao's wavelet analysis code v2\n",
    "\"\"\"\n",
    "Rule of thumb for the shape of numpy.ndarray: Frequency Dimension (Nf) -> Time Dimension (Nt) -> Component Dimension (3, or 3 x 3 / 6 x 3 for spectral maxtrix)\n",
    "\"\"\"\n",
    "\n",
    "def wavelet_coef_psd(time: np.ndarray, signal: np.ndarray, scales: np.ndarray, bandwidth: float = 6.0, downsample: int = 1, downsample_signal: bool = True):\n",
    "    \"\"\"\n",
    "    Compute complex Morlet wavelet coefficients and power spectral density (PSD).\n",
    "    Complex Morlet wavelet can be written as:\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    time : np.ndarray\n",
    "        Time vector of shape (Nt,). Can be in seconds or `np.datetime64`.\n",
    "    signal : np.ndarray\n",
    "        Input signal of shape (Nt,).\n",
    "    scales : np.ndarray\n",
    "        Wavelet scales corresponding to frequencies of shape (Nf,).\n",
    "    bandwidth : float, optional\n",
    "        bandwidthandwidth parameter for the Morlet wavelet (default is 6.0).\n",
    "    downsample : int, optional\n",
    "        Downsampling factor for the output (default is 1, meaning no downsampling).\n",
    "    downsample_signal : bool, optional\n",
    "        If True, downsample the input signal before computing wavelet coefficients (default is False).\n",
    "        If False, the signal is not downsampled, but the output coefficients and PSD are downsampled.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    time : np.ndarray\n",
    "        Time vector after optional downsampling.\n",
    "    frequency : np.ndarray\n",
    "        Frequencies corresponding to the wavelet scales.\n",
    "    coef : np.ndarray\n",
    "        Complex wavelet coefficients of shape (Nf, Nt), where Nf is the number of frequencies.\n",
    "    psd : np.ndarray\n",
    "        Power spectral density of shape (Nf, Nt).\n",
    "    signal : np.ndarray\n",
    "        Downsampled (moving average) signal if downsampling is applied.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The function uses `scipy.signal.cwt` with the Morlet wavelet by default.\n",
    "    - `scipy.signal.cwt` is deprecated in SciPy 1.12 and will be removed in SciPy 1.15. Alternatives like PyWavelets or ssqueezepy can be used.\n",
    "    - The power spectral density (PSD) is computed as the squared magnitude of the wavelet coefficients, scaled by `2 * dt`.\n",
    "    - Downsampling is applied to the time, coefficients, PSD, and signal if `downsample > 1`.\n",
    "    \"\"\"\n",
    "\n",
    "    if downsample_signal:\n",
    "        time = time[::downsample]\n",
    "        signal = signal[::downsample]\n",
    "        downsample = 1\n",
    "\n",
    "    if isinstance(time[0], np.datetime64):\n",
    "        elapsed_time = np.array(time).astype('datetime64[ns]').astype('float') / 1e9\n",
    "    else:\n",
    "        elapsed_time = np.array(time)\n",
    "\n",
    "    dt = elapsed_time[1] - elapsed_time[0]\n",
    "\n",
    "    # === Option 1: scipy.signal implementation ===\n",
    "    # bandwidthut, scipy.signal.cwt is deprecated in SciPy 1.12 and will be removed in SciPy 1.15. \n",
    "    # They recommend using PyWavelets instead.\n",
    "    # https://docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.signal.cwt.html\n",
    "    # However, as you can see in the bottom code block, pywt.cwt is actually problematic for the Morlet wavelet.\n",
    "\n",
    "    # widths = bandwidth * scales / (2 * np.pi)\n",
    "    # coef = scipy.signal.cwt(\n",
    "    #     signal,\n",
    "    #     scipy.signal.morlet2,\n",
    "    #     widths = widths,\n",
    "    #     w = bandwidth,\n",
    "    #     dtype = np.complex128\n",
    "    # )\n",
    "    # frequency = 1 / dt / scales\n",
    "\n",
    "    # === Option 2: pywt implementation ===\n",
    "    # Unlike scipy.signal.cwt, pywt.cwt is not L2-normalized.\n",
    "    # You need to multiply the coefficients by a factor to make them L2-normalized.\n",
    "    # Another two problems with pywt.cwt: The precision of the wavelet may be unenough while you can still not adjust the precision yourself.\n",
    "    # Check https://github.com/PyWavelets/pywt/issues/531\n",
    "    # This defect has been proposed at 2019 but still not fixed yet.\n",
    "\n",
    "    # central_frequency = 1.0\n",
    "    # wavelet = 'cmor%.1f-%.1f' % (bandwidth, central_frequency)\n",
    "    # coef, frequency = pywt.cwt(signal, scales, wavelet, dt, method = 'fft')\n",
    "    # coef *= np.sqrt(np.sqrt(bandwidth) * np.sqrt(2 * np.pi))  # amplitude normalization for Morlet\n",
    "\n",
    "    # === Option 3: ssqueezepy implementation (default used here) ===\n",
    "    # This one is accurate and fast. They claim that this package is the fastest implementation of the wavelet transform in Python.\n",
    "    # bandwidthut, this implementation is not elegantly designed and the input parameters are not well documented.\n",
    "    # Also, it requires package numba, which may raise some compatibility issues.\n",
    "\n",
    "    coef, scales = ssqueezepy.cwt(signal, ('morlet', {'mu': bandwidth}), scales = bandwidth / (2 * np.pi) * scales.astype(np.float32), fs = 1 / dt, l1_norm = False)\n",
    "    frequency = bandwidth / (2 * np.pi) / dt / scales\n",
    "\n",
    "    psd = (np.abs(coef) ** 2) * (2 * dt)\n",
    "\n",
    "\n",
    "    return time[::downsample], frequency, coef[:, ::downsample], psd[:, ::downsample], signal[::downsample]\n",
    "\n",
    "def wfft_coef_psd(time: np.ndarray, signal: np.ndarray, step: int = 1, window: int = 120):\n",
    "    \"\"\"\n",
    "    Compute short-time Fourier transform (STFT) coefficients and power spectral density (PSD) using a sliding Hanning window.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    time : np.ndarray\n",
    "        Time vector of shape (Nt,). Can be in seconds or `np.datetime64`.\n",
    "    signal : np.ndarray\n",
    "        Input signal of shape (Nt,).\n",
    "    step : int, optional\n",
    "        Step size for sliding the window (default is 1).\n",
    "    window : int, optional\n",
    "        Window length in samples (default is 120).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    wtime : np.ndarray\n",
    "        Center time for each window after sliding.\n",
    "    freq : np.ndarray\n",
    "        Frequency vector corresponding to the FFT.\n",
    "    coef : np.ndarray\n",
    "        Complex FFT coefficients of shape (Nf, Nt), where Nf is the number of frequencies.\n",
    "    psd : np.ndarray\n",
    "        Power spectral density of shape (Nf, Nt).\n",
    "    wsignal : np.ndarray\n",
    "        Window-averaged signal of shape (Nt,).\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The Hanning window is applied to each segment, and normalization is performed based on Parseval's theorem.\n",
    "    - The PSD is computed as the squared magnitude of the FFT coefficients, scaled by `2 * dt / window`.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(time[0], np.datetime64):\n",
    "        elapsed_time = np.array(time).astype('datetime64[ns]').astype('float') / 1e9\n",
    "    else:\n",
    "        elapsed_time = np.array(time)\n",
    "\n",
    "    dt = elapsed_time[1] - elapsed_time[0]\n",
    "\n",
    "    # Apply sliding window view\n",
    "    wtime = np.lib.stride_tricks.sliding_window_view(elapsed_time, window)[::step][:, 0] + dt * window / 2\n",
    "    freq = np.fft.fftfreq(window, dt)[:window // 2]\n",
    "    wsignal = np.lib.stride_tricks.sliding_window_view(signal, window)[::step]\n",
    "\n",
    "    # Apply Hanning window and normalize based on Parseval's theorem\n",
    "    wsignal = wsignal * np.sqrt(8 / 3) * np.hanning(window)\n",
    "\n",
    "    coef = np.fft.fft(wsignal, axis=-1)[:, :window // 2].T\n",
    "    psd = (np.abs(coef) ** 2) * 2 * dt / window\n",
    "\n",
    "    if isinstance(time[0], np.datetime64):\n",
    "        wtime = (np.array(wtime) * 1e9).astype('datetime64[ns]')\n",
    "\n",
    "    wsignal = np.mean(wsignal, axis=-1)\n",
    "\n",
    "    return wtime, freq, coef, psd, wsignal\n",
    "\n",
    "\n",
    "def svd_wave_analysis(coef: np.ndarray, freq_window: int = 5, time_window: int = 5):\n",
    "    \"\"\"\n",
    "    Perform SVD-based wave polarization analysis to compute planarity, ellipticity, and coherence.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    coef : np.ndarray\n",
    "        Complex coefficient tensor of shape (Nf, Nt, 3), where Nf is the number of frequencies, Nt is the number of time points, and 3 represents the 3 components.\n",
    "    freq_window : int, optional\n",
    "        Frequency-domain smoothing window size (default is 5).\n",
    "    time_window : int, optional\n",
    "        Time-domain smoothing window size (default is 5).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    planarity : np.ndarray\n",
    "        Planarity of the wave of shape (Nf, Nt), defined as `1 - sqrt(s3 / s1)`, where s1 and s3 are the largest and smallest singular values.\n",
    "    ellipticity_along_k: np.ndarray\n",
    "        Ellipticity along the wave vector direction of shape (Nf, Nt), defined as the ratio of the second to the first singular value.\n",
    "    coherence : np.ndarray\n",
    "        Coherence between the first and second principal components (along vh1 and vh2) of shape (Nf, Nt), computed from the smoothed wavefield spectrum.\n",
    "    degree_of_polarization : np.ndarray\n",
    "        3D Degree of polarization of shape (Nf, Nt), defined as `sqrt[3 / 2 * tr(J^2) / tr^2(J) - 1 / 2]`, computed using the wavefield spectrum.\n",
    "    vh : np.ndarray\n",
    "        Right singular vectors of shape (Nf, Nt, 3, 3), representing the polarization basis.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The input coefficients are smoothed in both frequency and time domains before performing SVD.\n",
    "    - The coherence is computed from the wavefield spectrum in the transformed basis.\n",
    "    \"\"\"\n",
    "\n",
    "    spec = np.einsum('ijk,ijl->ijkl', coef, coef.conj())\n",
    "    spec = bn.move_mean(spec, window=freq_window, min_count=1, axis=0)\n",
    "    spec = bn.move_mean(spec, window=time_window, min_count=1, axis=1)\n",
    "\n",
    "    spec_63 = np.concatenate([spec.real, spec.imag], axis=-2)\n",
    "    u, s, vh = np.linalg.svd(spec_63, full_matrices=False)\n",
    "\n",
    "    planarity = 1 - np.sqrt(s[:, :, 2] / s[:, :, 0])\n",
    "    ellipticity_along_k = s[:, :, 1] / s[:, :, 0]\n",
    "\n",
    "    # Rotate the coefficients to the wave frame, in which the third component is the least significant\n",
    "    coef_wf = np.einsum('ijk,ijlk->ijl', coef, vh)\n",
    "    spec_wf = np.einsum('ijk,ijl->ijkl', coef_wf, coef_wf.conj())\n",
    "    spec_wf = bn.move_mean(spec_wf, window=freq_window, min_count=1, axis=0)\n",
    "    spec_wf = bn.move_mean(spec_wf, window=time_window, min_count=1, axis=1)\n",
    "\n",
    "    # There are two ways to compute the degree of polarization:\n",
    "    # To see the difference in theory, please refer to the paper by Taubenschuss and Santonlik (2019).\n",
    "    # Equation (28) in Taubenschuss and Santonlik 2019: \n",
    "    # degree_of_polarization = np.sqrt(3 / 2 * np.abs(np.trace(np.matmul(spec, spec), axis1 = 2, axis2 = 3) / (np.trace(spec, axis1 = 2, axis2 = 3) ** 2)) - 1 / 2)\n",
    "\n",
    "    # Equation (74) in Taubenschuss and Santonlik 2019:\n",
    "    # Be careful about np.linalg.eigh, which returns the eigenvalues in ascending order\n",
    "    # While np.linalg.svd returns the singular values in descending order  \n",
    "    w, v = np.linalg.eigh(spec)\n",
    "    degree_of_polarization = (w[:, :, 2] - w[:, :, 1]) / np.sum(w, axis = -1)\n",
    "\n",
    "    coherence = np.abs(spec_wf[:, :, 0, 1]) / np.sqrt(np.abs(spec_wf[:, :, 0, 0] * spec_wf[:, :, 1, 1]))\n",
    "\n",
    "    return planarity, ellipticity_along_k, coherence, degree_of_polarization, vh\n",
    "\n",
    "\n",
    "def fac_wave_analysis(coef: np.ndarray, magf: np.ndarray):\n",
    "    \"\"\"\n",
    "    Project wave coefficients into the field-aligned coordinate (FAC) system.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    coef : np.ndarray\n",
    "        Wavelet or FFT coefficient tensor of shape (Nf, Nt, 3), where Nf is the number of frequencies, Nt is the number of time points, and 3 represents the 3 components.\n",
    "    field : np.ndarray\n",
    "        Reference magnetic field vector of shape (Nt, 3).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    compressibility : np.ndarray\n",
    "        Ratio of parallel power to the total power, indicating the compressibility of the wave.\n",
    "    ellipticity_along_b : np.ndarray\n",
    "        Ellipticity along the magnetic field direction, defined as the ratio of left-hand to right-hand polarized power.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The FAC system is defined using the magnetic field vector as the parallel direction, and two perpendicular directions are computed using cross products.\n",
    "    - The left-hand and right-hand polarized components are computed in the perpendicular plane.\n",
    "    \"\"\"\n",
    "    dir_para = (magf.T / np.linalg.norm(magf, axis = 1)).T\n",
    "    # Find the reference direction that is furthest from the magnetic field direction\n",
    "    dir_ref = np.eye(3)[np.argmin(np.abs(dir_para), axis = 1)]\n",
    "\n",
    "    dir_perp_1 = np.cross(dir_para, dir_ref)\n",
    "    dir_perp_1 = (dir_perp_1.T / np.linalg.norm(dir_perp_1, axis = 1)).T\n",
    "\n",
    "    dir_perp_2 = np.cross(dir_para, dir_perp_1)\n",
    "    dir_perp_2 = (dir_perp_2.T / np.linalg.norm(dir_perp_2, axis = 1)).T\n",
    "\n",
    "    coef_para = np.einsum('ijk,jk->ij', coef, dir_para)\n",
    "    coef_perp1 = np.einsum('ijk,jk->ij', coef, dir_perp_1)\n",
    "    coef_perp2 = np.einsum('ijk,jk->ij', coef, dir_perp_2)\n",
    "\n",
    "    coef_lh = (coef_perp1 - 1j * coef_perp2) / np.sqrt(2)\n",
    "    coef_rh = (coef_perp1 + 1j * coef_perp2) / np.sqrt(2)\n",
    "\n",
    "    ellipticity_along_b = (np.abs(coef_rh) - np.abs(coef_lh)) / (np.abs(coef_rh) + np.abs(coef_lh))\n",
    "    compressibility = np.abs(coef_para) ** 2 / (np.abs(coef_para) ** 2 + np.abs(coef_lh) ** 2 + np.abs(coef_rh) ** 2)\n",
    "\n",
    "    return compressibility, ellipticity_along_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdf8b71-b8e0-4bfd-8fd7-380a39341fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in full time-res magnetometer data\n",
    "dir = \"/Users/atcushen/Documents/MercuryModelling/MAG_highres/\"\n",
    "#folder_ls = [\"2015-03-27\",\"2015-03-29\",\"2015-04-05\",\"2015-04-06\"] # Events of interest\n",
    "folder_ls = [\"2015-03-20\",\"2015-03-21\",\"2015-03-22\",\"2015-03-23\",\"2015-03-24\",\"2015-03-25\", # All final orbits\n",
    "             \"2015-03-26\",\"2015-03-27\",\"2015-03-28\",\"2015-03-29\",\"2015-03-30\",\"2015-03-31\",\n",
    "             \"2015-04-01\",\"2015-04-02\",\"2015-04-03\",\"2015-04-04\",\"2015-04-05\",\"2015-04-06\",\n",
    "             \"2015-04-07\",\"2015-04-08\",\"2015-04-09\",\"2015-04-10\",\"2015-04-11\",\"2015-04-12\"]\n",
    "#folder_ls = [\"2011-04-09\"] # Event from Boardsen et al (2015)\n",
    "\n",
    "\n",
    "df = read_mag(str(dir+folder_ls[0]),highres=True)\n",
    "for day in folder_ls[1:]:\n",
    "    #print(day)\n",
    "    df = pd.concat([df,read_mag(str(dir+day),highres=True)], ignore_index=True)\n",
    "\n",
    "df_cs = (extract_cs(df,criteria = 'Bx', threshold = 0.5)).reset_index() #threshold = 0.3\n",
    "df_cs = calculate_terms(df_cs,use_model_data=False)\n",
    "\n",
    "# Look for where the DRs are spread\n",
    " # Determine which paths are in front of the planet vs. behind\n",
    "df_noview =df[(df['X']>0)]\n",
    "df_inview =df[(df['X']<0)]# & (df['timestamp']>pd.to_datetime(\"2015-04-01 21:00:00\")) & (df['timestamp']<pd.to_datetime(\"2015-04-03 13:00:00\"))]\n",
    "\n",
    "# Define figure and patches\n",
    "fig, ax = plt.subplots(ncols=1,figsize=(9,3))\n",
    "p0 = plt.Circle((0, 0), 1, color='black')\n",
    "p1 = plt.Circle((0, 0), 0.99, color='lightgrey')\n",
    "\n",
    "#plot0=ax.scatter(df_noview[\"Y\"]/R_M,df_noview[\"Z\"]/R_M,s=0.01,color='black',alpha=0.5)\n",
    "\n",
    "# Put in the planet\n",
    "ax.add_artist(p0)\n",
    "ax.add_artist(p1)\n",
    "\n",
    "# plot\n",
    "#plot1=ax.scatter(df_inview[\"Y\"]/R_M,df_inview[\"Z\"]/R_M,s=0.01,color='black',alpha=0.5)#,c=list((df_inview['DAY_OF_YEAR']-np.min(df_inview['DAY_OF_YEAR']))),\n",
    "ax.plot([-1,1],[0.2,0.2],linestyle=\"dashed\",color=\"black\")\n",
    "for pass_num in range(np.max(df_cs['cs_pass'])+1):\n",
    "    df_pass = df_cs[(df_cs['cs_pass']==pass_num)]\n",
    "    midpoint = [df_pass[\"Y\"][df_pass.index[len(df_pass.index)//2]]/R_M, df_pass[\"Z\"][df_pass.index[len(df_pass.index)//2]]/R_M]\n",
    "    plot2=ax.scatter(df_pass[\"Y\"]/R_M,df_pass[\"Z\"]/R_M,s=1.5,c = df_pass[\"B_z\"]-df_pass[\"B_zdip\"],vmin=-50,vmax=50,cmap='bwr')\n",
    "    #ax.plot([midpoint[0]-0.008,midpoint[0]+0.008],[midpoint[1],midpoint[1]],lw=1.8,color=\"yellow\")\n",
    "    \n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.set_xlim(-1.0,1.0)\n",
    "ax.set_ylim(-0.15,0.55)\n",
    "ax.invert_xaxis()\n",
    "ax.set_xlabel(\"Y [$R_M$]\")\n",
    "ax.set_ylabel(\"Z [$R_M$]\")\n",
    "clb1 = fig.colorbar(plot2, ax=ax)\n",
    "clb1.ax.set_title(\"$B_{z1}$ [nT]\")\n",
    "ax.set_title(str(\"Current sheet crossings:\"+str(df_cs['timestamp'][0])[:-7]+\" - \"+str(df_cs['timestamp'][len(df_cs)-1])[:-7]))\n",
    "plt.savefig(\"DR_distribution.png\",dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0286ffe4-61d9-4c12-891c-7e34bf2e820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview plots for paper, using wavelet code v2\n",
    "#INPUT\n",
    "time_average = 10*20 # how many time steps to average over (20 timesteps per sec * num of seconds)\n",
    "DR_event = 22#[22,27,46,48,51]\n",
    "\n",
    "psd_threshold = 1e-2\n",
    "\n",
    "show_legend = True\n",
    "DR_title_number = '1'\n",
    "\n",
    "# First, compute average By for the orbits before and after\n",
    "B_y_data = []\n",
    "\n",
    "# Declare our figure\n",
    "#fig, axs = plt.subplots(ncols=1, nrows=3,figsize=(12,15))\n",
    "\n",
    "# Iterate through each pass\n",
    "for event in np.unique(df_cs['cs_pass'])[DR_event-5:DR_event+5]:\n",
    "    indices = np.where(df_cs['cs_pass']==event)[0]\n",
    "\n",
    "    # Unpack data\n",
    "    Z = df_cs['Z'][indices].to_numpy()\n",
    "    t = df_cs['TIME_TAG'][indices].to_numpy()\n",
    "    t = t - t[0]\n",
    "    B_y = uniform_filter1d(df_cs['B_y'][indices],size = time_average)\n",
    "\n",
    "    # Save an entry for Z and Bz1 for each pass\n",
    "    B_y_data.append([t, B_y])\n",
    "    #axs[0].plot(t, B_y, color='black')\n",
    "\n",
    "# Compute terms for the dr event chosen\n",
    "indices = np.where(df_cs['cs_pass']==DR_event)[0]\n",
    "\n",
    "# Unpack the data\n",
    "X = df_cs['X'][indices].to_numpy() #[km]\n",
    "Y = df_cs['Y'][indices].to_numpy()\n",
    "Z = df_cs['Z'][indices].to_numpy()\n",
    "timestamp = df_cs['timestamp'][indices].to_numpy()\n",
    "t = df_cs['TIME_TAG'][indices].to_numpy()\n",
    "t = t - t[0]\n",
    "B_x = uniform_filter1d(df_cs['B_x'][indices],size = time_average) #[nT]\n",
    "B_y = uniform_filter1d(df_cs['B_y'][indices],size = time_average)\n",
    "B_z = uniform_filter1d(df_cs['B_z'][indices],size = time_average)\n",
    "B_xdip = calc_B_xdip(X,Y,Z)\n",
    "B_ydip = calc_B_ydip(X,Y,Z)\n",
    "B_zdip = calc_B_zdip(X,Y,Z)\n",
    "B_mag = np.sqrt(B_x**2+B_y**2+B_z**2)\n",
    "B_magdip = np.sqrt(B_xdip**2+B_ydip**2+B_zdip**2)\n",
    "\n",
    "# Compute dipole-difference terms\n",
    "B_x1 = B_x - B_xdip\n",
    "B_y1 = B_y - B_ydip\n",
    "B_z1 = B_z - B_zdip\n",
    "\n",
    "# Compute differences\n",
    "dX = np.insert(np.diff(X),0,np.diff(X)[0]) #np.gradient(dX)\n",
    "dY = np.insert(np.diff(Y),0,np.diff(Y)[0]) #np.gradient(dY)\n",
    "dZ = np.insert(np.diff(Z),0,np.diff(Z)[0]) #np.gradient(dZ)\n",
    "dB_x = np.insert(np.diff(B_x),0,np.diff(B_x)[0]) # insert 0 at the front to fill the gap\n",
    "dB_y = np.insert(np.diff(B_y),0,np.diff(B_y)[0])\n",
    "dB_z = np.insert(np.diff(B_z),0,np.diff(B_z)[0])\n",
    "dB2 = np.insert(np.diff(B_mag**2),0,np.diff(B_mag**2)[0])\n",
    "\n",
    "# Fix the tiny dY using a mask to remove values where dY is very small\n",
    "mask = np.abs(dY)<1e-2\n",
    "\n",
    "# Compute tension subcomponents\n",
    "Cx1 = B_x*dB_x/dX\n",
    "Cx2 = B_y*dB_x/dY\n",
    "Cx3 = B_z*dB_x/dZ\n",
    "Cy1 = B_x*dB_y/dX\n",
    "Cy2 = B_y*dB_y/dY\n",
    "Cy3 = B_z*dB_y/dZ\n",
    "Cz1 = B_x*dB_z/dX\n",
    "Cz2 = B_y*dB_z/dY\n",
    "Cz3 = B_z*dB_z/dZ\n",
    "\n",
    "# Replace nans of all terms with 1/dY with 0\n",
    "Cx2[mask] = 0\n",
    "Cy2[mask] = 0\n",
    "Cz2[mask] = 0\n",
    "\n",
    "# Compute tension components, convert to SI (B in nT, dX in 1/km)\n",
    "T_x = 1/mu_0 * (Cx1+Cx2+Cx3) * (1e-9)**2 * 1e-3\n",
    "T_y = 1/mu_0 * (Cy1+Cy2+Cy3) * (1e-9)**2 * 1e-3\n",
    "T_z = 1/mu_0 * (Cz1+Cz2+Cz3) * (1e-9)**2 * 1e-3\n",
    "\n",
    "# Compute pressure gradient, convert to SI\n",
    "P_grad_x = -dB2 / dX / (2*mu_0) * (1e-9)**2 * (1e-3)\n",
    "P_grad_y = -dB2 / dY / (2*mu_0) * (1e-9)**2 * (1e-3)\n",
    "P_grad_z = -dB2 / dZ / (2*mu_0) * (1e-9)**2 * (1e-3)\n",
    "\n",
    "P_grad_y[mask] = 0\n",
    "\n",
    "# Project into spacecraft motion vector\n",
    "T_tot = uniform_filter1d((T_x*dX + T_y*dY + T_z*dZ)/np.sqrt(dX**2+dY**2+dZ**2), size = time_average)\n",
    "P_tot = uniform_filter1d((P_grad_x*dX+P_grad_y*dY+P_grad_z*dZ)/np.sqrt(dX**2+dY**2+dZ**2), size = time_average)\n",
    "\n",
    "# Compute Jy terms\n",
    "J_ydX = (-1/mu_0)*np.gradient(B_z,X) * 1e-9 * 1e-3 \n",
    "J_ydZ = (1/mu_0)*np.gradient(B_x,Z) * 1e-9 * 1e-3 \n",
    "J_ydipdX = (-1/mu_0)*np.gradient(B_zdip,X) * 1e-9 * 1e-3 #- dB_z1/dX\n",
    "J_ydipdZ = (1/mu_0)*np.gradient(B_xdip,Z) * 1e-9 * 1e-3 #- dB_z1/dX\n",
    "\n",
    "# Weight terms based on our direction of motion\n",
    "J_y = (J_ydX*dX + J_ydZ*dZ)/np.sqrt(dX**2+dZ**2)\n",
    "J_ydip = (J_ydipdX*dX + J_ydipdZ*dZ)/np.sqrt(dX**2+dZ**2)\n",
    "J_y_smooth = uniform_filter1d(J_y, size = time_average)\n",
    "J_ydip_smooth = uniform_filter1d(J_ydip, size = time_average)\n",
    "\n",
    "# Use interpolation to average over all events neighbouring the DR\n",
    "t_common, B_y_average, B_y_std = average_datasets_with_std(B_y_data, x_common = t)\n",
    "\n",
    "# Declare our figure\n",
    "fig, axs = plt.subplots(ncols=1, nrows=4,figsize=(4,10), sharex=True, height_ratios=[5,3,2,2])\n",
    "\n",
    "# Remove space between \n",
    "plt.subplots_adjust(hspace=0)\n",
    "\n",
    "# Plot magnetic field components\n",
    "#axs[0].plot(t, B_x, c = 'red', label='$B_x$')\n",
    "axs[0].plot(t, B_y - B_y_average, c = 'green', label = '$\\delta B_y$')\n",
    "axs[0].plot(t, B_z1, c = 'blue', label = '$B_{z1}$')\n",
    "axs[0].axhline(y=0, c = 'black', linestyle = 'dashed')\n",
    "\n",
    "# Tidy up axs[0]\n",
    "axs[0].grid('on')\n",
    "\n",
    "# Show B_x on separate axis if it's too different\n",
    "if DR_event in [48]:\n",
    "    #axs0_1 = axs[0].twinx()\n",
    "    #axs0_1.plot(t, B_x, c = 'red', label='$B_x$')\n",
    "    #axs0_1.set_ylabel(\"$B_x$ [nT]\", fontsize = 12, color='red')\n",
    "    #axs0_1.tick_params(axis='both',labelsize=12)\n",
    "    #axs0_1.tick_params(axis='y',color='red')\n",
    "    #axs0_1.set_ylim(0,325)\n",
    "    #axs[0].plot([-1e5,-1e5], [-1e5,-1e5], c = 'red', label='$B_x$')\n",
    "    print(\"not showing Bx\")\n",
    "else:\n",
    "    axs[0].plot(t, B_x, c = 'red', label='$B_x$')\n",
    "axs[0].set_ylim(-76,56)\n",
    "axs[0].set_ylabel(\"$B$ [nT]\", fontsize = 12)\n",
    "if show_legend:\n",
    "    axs[0].legend(loc = 'lower right', fontsize = 12, ncol=3)\n",
    "axs[0].tick_params(axis='both',labelsize=12)\n",
    "\n",
    "# Show force balance and Jy in second panel\n",
    "#axs[1].plot(t, T_tot*1e12, label = '$T_{mag}$',color='tab:blue')\n",
    "#axs[1].plot(t, P_tot*1e12, label = '$-grad(p_{mag})$',color='tab:orange')\n",
    "#axs[1].plot(t, (T_tot+P_tot)*1e12, label = 'Total',color='black')\n",
    "axs[1].axhline(y=0, c = 'black', linestyle = 'dashed')\n",
    "#axs[1].plot(0,1e10, color = 'tab:green', label = '$J_y$')\n",
    "#axs1_2 = axs[1].twinx()\n",
    "#axs1_2.plot(t, (J_y_smooth-J_ydip_smooth)*1e9, color = 'tab:green')\n",
    "axs[1].plot(t, uniform_filter1d((J_ydX-J_ydipdX)*1e9,size = time_average), color = 'tab:blue', label='$∂B_z/∂x$',lw=0.5)\n",
    "axs[1].plot(t, uniform_filter1d((J_ydZ-J_ydipdZ)*1e9,size = time_average), color = 'tab:red', label='$∂B_x/∂z$',lw=0.5)\n",
    "axs[1].plot(t, uniform_filter1d((J_ydX-J_ydipdX)*1e9+(J_ydZ-J_ydipdZ)*1e9,size = time_average), color = 'black',alpha = 1,lw=0.5)\n",
    "#axs[1].plot(t, gaussian_filter1d((J_ydZ-J_ydipdZ)*1e9, sigma=time_average), color = 'tab:red', linestyle='dashed')\n",
    "#axs[1].plot(t, (J_y_smooth-J_ydip_smooth)*1e9, color = 'black', label = 'weighted sum')\n",
    "#axs1_2.plot(t, J_ydip_smooth*1e9, color = 'tab:green', linestyle = 'dashed')\n",
    "\n",
    "# Tidy up axs[1]\n",
    "axs[1].grid('on')\n",
    "#axs[1].set_ylim(-0.4,0.4)\n",
    "axs[1].set_ylim(-2e3,2e3)\n",
    "#axs1_2.set_ylim(-500,500)\n",
    "#axs[1].set_ylabel(\"Force [pN/m$^3$]\", fontsize = 12)\n",
    "#axs1_2.set_ylabel(\"Current density [nA/m$^2$]\", fontsize = 12)\n",
    "axs[1].set_ylabel(\"$J_y$ [nA/m$^2$]\", fontsize = 12)\n",
    "axs[1].set_yscale('symlog', linthresh=1e2,linscale=1e0)\n",
    "axs[1].tick_params(axis='both',labelsize=12)\n",
    "if show_legend:\n",
    "    axs[1].legend(loc = 'lower right', fontsize = 12, ncol=2)\n",
    "#axs1_2.tick_params(axis='both',labelsize=12)\n",
    "\n",
    "# Denote start / end of DF:\n",
    "if DR_event in [47,48,60]:\n",
    "    t_start_guess = 200\n",
    "    t_stop_guess = 700 \n",
    "elif DR_event in [28,44]:\n",
    "    t_start_guess = 100\n",
    "    t_stop_guess = 600 \n",
    "else:\n",
    "    t_start_guess = 200 # when after this time does Bz1>0\n",
    "    t_stop_guess = 500 # when before this time does Bz1<0\n",
    "\n",
    "# Find the index of start/stop\n",
    "i_start = np.where((B_z1 > 0) & (t > t_start_guess))[0][0]\n",
    "i_stop = np.where((B_z1 > 0) & (t < t_stop_guess))[0][-1]\n",
    "\n",
    "# Highlight DR region\n",
    "i_middle = int((i_stop + i_start) / 2)\n",
    "for ax in axs:\n",
    "    ax.axvspan(t[i_start],t[i_stop],color='black',alpha = 0.05) # Bz1>0 region\n",
    "    ax.axvspan(t[i_middle - 60*20],t[i_middle + 60*20],color='black',alpha = 0.05) # FIPS sampling time\n",
    "    ax.axvline(t[i_cs],color='black', linestyle='dashed') # equator crossing\n",
    "print(\"DR start:\",timestamp[i_middle - 60*20],\"DR stop:\",timestamp[i_middle + 60*20])\n",
    "\n",
    "# Guess lower bound for cs crossing time\n",
    "t_cs_start_guess = 100\n",
    "\n",
    "# Find index of cs crossing\n",
    "i_cs = np.where((B_x > 0) & (t > t_cs_start_guess))[0][0]\n",
    "\n",
    "\n",
    "# Set time range\n",
    "total_plot_time = 150 + (t[i_stop] - t[i_start]) # Total length of time to plot\n",
    "t_before = 60 * 1.5 # How much time before DR starts to show\n",
    "i_plot_start = np.where(t > t[i_start]-t_before)[0][0]\n",
    "i_plot_stop = np.where(t < t[i_start]-t_before+total_plot_time)[0][-1]\n",
    "for ax in axs:\n",
    "    ax.set_xlim(t[i_plot_start], t[i_plot_stop])\n",
    "\n",
    "############## COMPUTE WAVE CHARACTERISTICS #############\n",
    "\n",
    "# Relabel data\n",
    "mag_ut = df_cs[\"timestamp\"][indices]\n",
    "mag_t = df_cs[\"TIME_TAG\"][indices] - df_cs[\"TIME_TAG\"][indices[0]]\n",
    "mag_bx = df_cs[\"B_x\"][indices]\n",
    "mag_by = df_cs[\"B_y\"][indices]\n",
    "mag_bz = df_cs[\"B_z\"][indices]\n",
    "mag_bt = mag_bt = np.sqrt(mag_bx**2+mag_by**2+mag_bz**2)\n",
    "mag_bz1 = mag_bz - calc_B_zdip(df_cs[\"X\"][indices],df_cs[\"Y\"][indices],df_cs[\"Z\"][indices])\n",
    "\n",
    "#start_ut = np.datetime64('2015-03-29T12:12:00')\n",
    "#end_ut = np.datetime64('2015-03-29T12:16:00')\n",
    "\n",
    "# Convert from np index to pd index for start/stop times\n",
    "start_idx = np.searchsorted(mag_ut, timestamp[0])\n",
    "end_idx = np.searchsorted(mag_ut, timestamp[-1])\n",
    "\n",
    "\n",
    "sig_t = np.copy(mag_t[start_idx:end_idx])\n",
    "sig_ut = np.copy(mag_ut[start_idx:end_idx])\n",
    "sig_bx = np.copy(mag_bx[start_idx:end_idx])\n",
    "sig_by = np.copy(mag_by[start_idx:end_idx])\n",
    "sig_bz = np.copy(mag_bz[start_idx:end_idx])\n",
    "sig_bt = np.copy(mag_bt[start_idx:end_idx])\n",
    "sig_bz1 = np.copy(mag_bz1[start_idx:end_idx])\n",
    "\n",
    "sig_ut = np.copy(mag_ut[start_idx:end_idx])\n",
    "sig_bx = np.copy(mag_bx[start_idx:end_idx])\n",
    "sig_by = np.copy(mag_by[start_idx:end_idx])\n",
    "sig_bz = np.copy(mag_bz[start_idx:end_idx])\n",
    "sig_bt = np.copy(mag_bt[start_idx:end_idx])\n",
    "sig_bz1 = np.copy(mag_bz1[start_idx:end_idx])\n",
    "\n",
    "import scipy.signal\n",
    "sos = scipy.signal.butter(10, 0.1, 'hp', fs=20, output='sos')\n",
    "\n",
    "filtered_sig_bx = scipy.signal.sosfiltfilt(sos, sig_bx)\n",
    "filtered_sig_by = scipy.signal.sosfiltfilt(sos, sig_by)\n",
    "filtered_sig_bz = scipy.signal.sosfiltfilt(sos, sig_bz)\n",
    "filtered_sig_bt = scipy.signal.sosfiltfilt(sos, sig_bt)\n",
    "\n",
    "use_wavelet = True\n",
    "\n",
    "if use_wavelet:\n",
    "    bandwidth = 6.0\n",
    "    # log-scale\n",
    "    scales = np.power(2, np.linspace(1, 7, 1000))\n",
    "    # linear-scale (for comparing with the windowed FFT)\n",
    "    #scales = np.arange(2, 400)\n",
    "    downsample = 1 # For comparing with the windowed FFT with a step of 50\n",
    "    freq_window = 3\n",
    "    time_window = 7\n",
    "    downsample_signal = False # Downsample the signal before computing wavelet coefficients (True) or Downsample the coefficients after computing (False)\n",
    "\n",
    "    time, frequency, coef_bx, psd_bx, avg_bx = wavelet_coef_psd(sig_ut, sig_bx, scales, bandwidth, downsample = downsample, downsample_signal = downsample_signal)\n",
    "    _, _, coef_by, psd_by, avg_by = wavelet_coef_psd(sig_ut, sig_by, scales, bandwidth, downsample = downsample, downsample_signal = downsample_signal)\n",
    "    _, _, coef_bz, psd_bz, avg_bz = wavelet_coef_psd(sig_ut, sig_bz, scales, bandwidth, downsample = downsample, downsample_signal = downsample_signal)\n",
    "    _, _, coef_bt, psd_bt, avg_bt = wavelet_coef_psd(sig_ut, sig_bt, scales, bandwidth, downsample = downsample, downsample_signal = downsample_signal)\n",
    "    coi = (np.sqrt(2) * bandwidth / (2 * np.pi) / frequency).astype(float)\n",
    "    if isinstance(sig_ut[0], np.datetime64):\n",
    "        coi = (1e9 * coi).astype('timedelta64[ns]')\n",
    "\n",
    "else: # Use windowed FFT\n",
    "    step = 1\n",
    "    window = 400\n",
    "    \n",
    "    freq_window = 3\n",
    "    time_window = 7\n",
    "\n",
    "    time, frequency, coef_bx, psd_bx, avg_bx = wfft_coef_psd(sig_ut, sig_bx, step = step, window = window)\n",
    "    _, _, coef_by, psd_by, avg_by = wfft_coef_psd(sig_ut, sig_by, step = step, window = window)\n",
    "    _, _, coef_bz, psd_bz, avg_bz = wfft_coef_psd(sig_ut, sig_bz, step = step, window = window)\n",
    "    _, _, coef_bt, psd_bt, _ = wfft_coef_psd(sig_ut, sig_bt, step = step, window = window)\n",
    "    \n",
    "\n",
    "coef = np.array([coef_bx, coef_by, coef_bz]).transpose(1, 2, 0)\n",
    "avg = np.array([avg_bx, avg_by, avg_bz]).T\n",
    "magf_direction = ((avg.T) / np.linalg.norm(avg, axis = 1)).T\n",
    "\n",
    "psd = (psd_bx + psd_by + psd_bz)\n",
    "\n",
    "compressibility = (psd_bt) / psd\n",
    "\n",
    "compressibility, ellipticity_along_b = fac_wave_analysis(coef, magf_direction)\n",
    "\n",
    "_mask_idx = np.where(psd < psd_threshold)\n",
    "compressibility[_mask_idx] = np.nan\n",
    "ellipticity_along_b[_mask_idx] = np.nan\n",
    "\n",
    "# Convert to time [s] for plotting\n",
    "ts = np.array((time-time[0])/1e9,dtype = 'float')\n",
    "\n",
    "pc2 = axs[2].pcolormesh(ts, frequency, np.log10(psd), cmap='jet', shading='auto', vmax = 1.0, vmin = -3)\n",
    "\n",
    "axs[2].grid('on')\n",
    "axs[2].set_yscale('log')\n",
    "axs[2].set_ylim(0.2, 5.0)\n",
    "\n",
    "\n",
    "pc3 = axs[3].pcolormesh(ts, frequency, compressibility, cmap='jet', vmax = 1.0, vmin = 0)\n",
    "axs[3].grid('on')\n",
    "axs[3].set_yscale('log')\n",
    "axs[3].set_ylim(0.2, 5.0)\n",
    "\n",
    "#pc4 = axs[4].pcolormesh(ts, frequency, ellipticity_along_b, cmap='coolwarm', vmax = 1.0, vmin = -1.0)\n",
    "#axs[4].grid('on')\n",
    "#axs[4].set_yscale('log')\n",
    "#axs[4].set_ylim(0.2, 5.0)\n",
    "\n",
    "# Add colorbars\n",
    "cbar_ax2 = fig.add_axes(\n",
    "    [axs[2].get_position().x1 + 0.01, axs[2].get_position().y0 + 0.01, 0.02, axs[2].get_position().y1 - axs[2].get_position().y0 - 0.02])\n",
    "cbar2 = fig.colorbar(pc2, cax=cbar_ax2)\n",
    "# cbar2.set_label('$\\mathrm{log_{10}(PSD)}$\\n($\\mathrm{nT^2/Hz}$)')\n",
    "cbar2.ax.tick_params(which='major', length=4, direction='in', pad=3)\n",
    "cbar2.set_label(\"PSD [nT$^2$/Hz]\", rotation=90,fontsize = 12)\n",
    "\n",
    "cbar_ax3 = fig.add_axes(\n",
    "    [axs[3].get_position().x1 + 0.01, axs[3].get_position().y0 + 0.01, 0.02, axs[3].get_position().y1 - axs[3].get_position().y0 - 0.02])\n",
    "cbar3 = fig.colorbar(pc3, cax=cbar_ax3)\n",
    "# cbar3.set_label('Compre\\n-ssibility')\n",
    "cbar3.ax.tick_params(which='major', length=4, direction='in', pad=3)\n",
    "cbar3.set_label(\"Compressibility\", rotation=90,fontsize = 12)\n",
    "\n",
    "#cbar_ax4 = fig.add_axes(\n",
    "#    [axs[4].get_position().x1 + 0.01, axs[4].get_position().y0 + 0.01, 0.02, axs[4].get_position().y1 - axs[4].get_position().y0 - 0.02])\n",
    "#cbar4 = fig.colorbar(pc4, cax=cbar_ax4)\n",
    "## cbar3.set_label('Compre\\n-ssibility')\n",
    "#cbar4.ax.tick_params(which='major', length=4, direction='in', pad=3)\n",
    "#cbar4.set_label(\"Ellipticity\", rotation=90,fontsize = 12)\n",
    "\n",
    "# Add axes labels\n",
    "axs[2].set_ylabel(\"Freq. [Hz]\",fontsize=12)\n",
    "axs[3].set_ylabel(\"Freq. [Hz]\",fontsize=12)\n",
    "#axs[4].set_ylabel(\"Freq. [Hz]\",fontsize=12)\n",
    "\n",
    "############\n",
    "\n",
    "# Fix Horizontal axis ticks:\n",
    "# Find nearest minute to start time\n",
    "# Timestamp is always 29 characters long, so we can use a string method:\n",
    "tick_indices = []#[i_plot_start]\n",
    "tick_positions = []#[t[i_plot_start]]\n",
    "tick_labels = []#[\"HH:MM:SS   \\n$X_{MSM}$ [$R_M$]   \\n$Y_{MSM}$ [$R_M$]   \\n$Z_{MSM}$ [$R_M$]   \"]\n",
    "\n",
    "for i in np.arange(i_plot_start,i_plot_stop):\n",
    "    if str(timestamp[i])[-12:-7] == '00.04' or str(timestamp[i])[-12:-7] == '00.03' or str(timestamp[i])[-12:-7] == '00.02' or str(timestamp[i])[-12:-7] == '00.01' or str(timestamp[i])[-12:-7] == '00.00':\n",
    "        tick_indices.append(i)\n",
    "        tick_positions.append(t[i])\n",
    "        tick_labels.append(str(str(timestamp[i])[11:-13]+'\\n'+str(round(X[i]/R_M,3))+'\\n'+str(round(Y[i]/R_M,3))+'\\n'+str(round(Z[i]/R_M-0.2,3))))\n",
    "\n",
    "# Show major ticks\n",
    "axs[3].set_xticks(tick_positions, tick_labels, fontsize=12)\n",
    "\n",
    "# Show minor ticks\n",
    "minor_ticks = np.arange(t[tick_indices[1]-20*60], np.max(t), 15)\n",
    "minor_locator = FixedLocator(minor_ticks)\n",
    "axs[3].xaxis.set_minor_locator(minor_locator)\n",
    "\n",
    "# Set title:\n",
    "axs[0].set_title(str(\"DR\"+DR_title_number+\": \"+str(timestamp[i_plot_start])[:10]),fontsize=12)\n",
    "\n",
    "fig.savefig(str('DR_'+str(DR_event)+'_overview.png'),bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "#print(f'Common x: {x_common}')\n",
    "#print(f'Average z: {z_average}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
